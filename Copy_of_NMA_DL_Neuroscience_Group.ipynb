{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of NMA_DL_Neuroscience_Group.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RiO2yUzom-eO",
        "outputId": "9840d597-91fc-47bc-855d-7ef33bb9b6d1"
      },
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install mindscope_utilities --upgrade\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install dill\n",
        "!pip install pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 6.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.2.4\n",
            "Collecting mindscope_utilities\n",
            "  Downloading mindscope_utilities-0.1.8.tar.gz (11 kB)\n",
            "Collecting flake8\n",
            "  Downloading flake8-3.9.2-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from mindscope_utilities) (3.6.4)\n",
            "Collecting allensdk\n",
            "  Downloading allensdk-2.12.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib<3.4.3,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (3.2.2)\n",
            "Requirement already satisfied: jinja2<2.12.0,>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (2.11.3)\n",
            "Collecting simpleitk<3.0.0,>=2.0.2\n",
            "  Downloading SimpleITK-2.1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 35 kB/s \n",
            "\u001b[?25hCollecting psycopg2-binary<3.0.0,>=2.7\n",
            "  Downloading psycopg2_binary-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 51.5 MB/s \n",
            "\u001b[?25hCollecting nest-asyncio==1.2.0\n",
            "  Downloading nest_asyncio-1.2.0-py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: scikit-image<0.17.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (0.16.2)\n",
            "Requirement already satisfied: seaborn<1.0.0 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (0.11.1)\n",
            "Requirement already satisfied: six<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (1.15.0)\n",
            "Collecting xarray<0.16.0\n",
            "  Downloading xarray-0.15.1-py3-none-any.whl (668 kB)\n",
            "\u001b[K     |████████████████████████████████| 668 kB 33.5 MB/s \n",
            "\u001b[?25hCollecting pynwb<2.0.0,>=1.3.2\n",
            "  Downloading pynwb-1.5.1-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 10.5 MB/s \n",
            "\u001b[?25hCollecting h5py<3.0.0,>=2.8\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 61.9 MB/s \n",
            "\u001b[?25hCollecting simplejson<4.0.0,>=3.10.0\n",
            "  Downloading simplejson-3.17.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 67.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future<1.0.0,>=0.14.3 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (0.16.0)\n",
            "Collecting argschema<4.0.0,>=3.0.1\n",
            "  Downloading argschema-3.0.1.tar.gz (26 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (2.13.0)\n",
            "Collecting pandas<=0.25.3,>=0.25.1\n",
            "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 55.0 MB/s \n",
            "\u001b[?25hCollecting scikit-build<1.0.0\n",
            "  Downloading scikit_build-0.12.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting ndx-events<=0.2.0\n",
            "  Downloading ndx_events-0.2.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (4.62.0)\n",
            "Collecting requests-toolbelt<1.0.0\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels<=0.13.0 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (0.10.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (1.4.1)\n",
            "Collecting pynrrd<1.0.0,>=0.2.1\n",
            "  Downloading pynrrd-0.4.2-py2.py3-none-any.whl (18 kB)\n",
            "Collecting numpy<1.19.0,>=1.15.4\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 131 kB/s \n",
            "\u001b[?25hCollecting aiohttp==3.7.4\n",
            "  Downloading aiohttp-3.7.4-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.0 MB/s \n",
            "\u001b[?25hCollecting glymur==0.8.19\n",
            "  Downloading Glymur-0.8.19.tar.gz (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 59.6 MB/s \n",
            "\u001b[?25hCollecting boto3==1.17.21\n",
            "  Downloading boto3-1.17.21-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 70.4 MB/s \n",
            "\u001b[?25hCollecting hdmf<2.5.0\n",
            "  Downloading hdmf-2.4.0-py2.py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from allensdk->mindscope_utilities) (2.23.0)\n",
            "Collecting tables<4.0.0,>=3.6.0\n",
            "  Downloading tables-3.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 55.6 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 68.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4->allensdk->mindscope_utilities) (3.7.4.3)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4->allensdk->mindscope_utilities) (3.0.4)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4->allensdk->mindscope_utilities) (21.2.0)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.21\n",
            "  Downloading botocore-1.20.112-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 55.6 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from glymur==0.8.19->allensdk->mindscope_utilities) (57.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from argschema<4.0.0,>=3.0.1->allensdk->mindscope_utilities) (3.13)\n",
            "Collecting marshmallow<4.0,>=3.0.0\n",
            "  Downloading marshmallow-3.13.0-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.21->boto3==1.17.21->allensdk->mindscope_utilities) (2.8.2)\n",
            "Collecting ruamel.yaml<1,>=0.15\n",
            "  Downloading ruamel.yaml-0.17.11-py3-none-any.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema<4,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from hdmf<2.5.0->allensdk->mindscope_utilities) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<2.12.0,>=2.7.3->allensdk->mindscope_utilities) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.4.3,>=1.4.3->allensdk->mindscope_utilities) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.4.3,>=1.4.3->allensdk->mindscope_utilities) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.4.3,>=1.4.3->allensdk->mindscope_utilities) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<=0.25.3,>=0.25.1->allensdk->mindscope_utilities) (2018.9)\n",
            "Collecting pynwb<2.0.0,>=1.3.2\n",
            "  Downloading pynwb-1.5.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 10.9 MB/s \n",
            "\u001b[?25h  Downloading pynwb-1.4.0-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->allensdk->mindscope_utilities) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->allensdk->mindscope_utilities) (2021.5.30)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 74.7 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.1.2\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.29.0 in /usr/local/lib/python3.7/dist-packages (from scikit-build<1.0.0->allensdk->mindscope_utilities) (0.37.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scikit-build<1.0.0->allensdk->mindscope_utilities) (21.0)\n",
            "Collecting distro\n",
            "  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.17.0,>=0.14.0->allensdk->mindscope_utilities) (2.6.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.17.0,>=0.14.0->allensdk->mindscope_utilities) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.17.0,>=0.14.0->allensdk->mindscope_utilities) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.17.0,>=0.14.0->allensdk->mindscope_utilities) (2.4.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels<=0.13.0->allensdk->mindscope_utilities) (0.5.1)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables<4.0.0,>=3.6.0->allensdk->mindscope_utilities) (2.7.3)\n",
            "Collecting pyflakes<2.4.0,>=2.3.0\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 9.8 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting pycodestyle<2.8.0,>=2.7.0\n",
            "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 893 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from flake8->mindscope_utilities) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->flake8->mindscope_utilities) (3.5.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->mindscope_utilities) (8.8.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->mindscope_utilities) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->mindscope_utilities) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->mindscope_utilities) (1.10.0)\n",
            "Building wheels for collected packages: mindscope-utilities, glymur, argschema\n",
            "  Building wheel for mindscope-utilities (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mindscope-utilities: filename=mindscope_utilities-0.1.8-py3-none-any.whl size=12266 sha256=0eb985af7039ddeafa1e208cfa0cedaeaf01f4dd5841653176decae2300e8af2\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/b9/b4/8016ed7e290354340fe051fed296fc33ad2586443b50db9d68\n",
            "  Building wheel for glymur (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glymur: filename=Glymur-0.8.19-py3-none-any.whl size=2722015 sha256=a98af20bbf130ba519d0de5439f02129391f94596c031f370fe2d0913bc8de88\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/bc/bd/1786279b44db4cbd6ded18090d02978fc31e78e3e516551bb4\n",
            "  Building wheel for argschema (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for argschema: filename=argschema-3.0.1-py2.py3-none-any.whl size=19074 sha256=7e6348a1ff93cf14b9fcbe92c2fec70f888435b00bf11950615f58a54aa56df5\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/bb/5a/1a452447de8b3d97044cee57bed5a8e3500c0d996b1e623e75\n",
            "Successfully built mindscope-utilities glymur argschema\n",
            "Installing collected packages: ruamel.yaml.clib, numpy, urllib3, ruamel.yaml, pandas, jmespath, h5py, multidict, hdmf, botocore, yarl, s3transfer, pynwb, marshmallow, distro, async-timeout, xarray, tables, simplejson, simpleitk, scikit-build, requests-toolbelt, pynrrd, pyflakes, pycodestyle, psycopg2-binary, nest-asyncio, ndx-events, mccabe, glymur, boto3, argschema, aiohttp, flake8, allensdk, mindscope-utilities\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: xarray\n",
            "    Found existing installation: xarray 0.18.2\n",
            "    Uninstalling xarray-0.18.2:\n",
            "      Successfully uninstalled xarray-0.18.2\n",
            "  Attempting uninstall: tables\n",
            "    Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "  Attempting uninstall: nest-asyncio\n",
            "    Found existing installation: nest-asyncio 1.5.1\n",
            "    Uninstalling nest-asyncio-1.5.1:\n",
            "      Successfully uninstalled nest-asyncio-1.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires h5py~=3.1.0, but you have h5py 2.10.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.18.5 which is incompatible.\n",
            "nbclient 0.5.4 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 0.25.3 which is incompatible.\n",
            "fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.2 requires xarray>=0.16.1, but you have xarray 0.15.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.7.4 allensdk-2.12.2 argschema-3.0.1 async-timeout-3.0.1 boto3-1.17.21 botocore-1.20.112 distro-1.6.0 flake8-3.9.2 glymur-0.8.19 h5py-2.10.0 hdmf-2.4.0 jmespath-0.10.0 marshmallow-3.13.0 mccabe-0.6.1 mindscope-utilities-0.1.8 multidict-5.1.0 ndx-events-0.2.0 nest-asyncio-1.2.0 numpy-1.18.5 pandas-0.25.3 psycopg2-binary-2.9.1 pycodestyle-2.7.0 pyflakes-2.3.1 pynrrd-0.4.2 pynwb-1.4.0 requests-toolbelt-0.9.1 ruamel.yaml-0.17.11 ruamel.yaml.clib-0.2.6 s3transfer-0.3.7 scikit-build-0.12.0 simpleitk-2.1.0 simplejson-3.17.3 tables-3.6.1 urllib3-1.25.11 xarray-0.15.1 yarl-1.6.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6NrcQbaFSl-"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import json\n",
        "\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from scipy.stats import zscore\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
        "from allensdk.brain_observatory.ecephys.visualization import plot_mean_waveforms, plot_spike_counts, raster_plot\n",
        "from allensdk.brain_observatory.visualization import plot_running_speed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lShX-yLqIZ4q"
      },
      "source": [
        "## Load the session and experiment summary tables\n",
        "\n",
        "The AllenSDK provides functionality for downloading tables that describe all session types ('brain_observatory_1.1', 'functional_connectivity') in the Visual Coding – Neuropixels. We have first download the data cache:\n",
        "\n",
        "\n",
        "- Brain Observatory 1.1 and Functional Connectivity sessions correspond to different stimulus sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ82E3eQnUpp",
        "outputId": "9effed2d-7b23-4db4-bc31-757b95a39ad7"
      },
      "source": [
        "# this path determines where downloaded data will be stored\n",
        "manifest_path = os.path.join('/local1/ecephys_cache_dir/', \"manifest.json\")\n",
        "#manifest_path = os.path.join('/temp', \"manifest.json\")\n",
        "\n",
        "cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n",
        "\n",
        "print(cache.get_all_session_types())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['brain_observatory_1.1', 'functional_connectivity']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lXYx3j9JRs7"
      },
      "source": [
        "Then we can access the session table directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Mr03FvJRs7"
      },
      "source": [
        "sessions = cache.get_session_table()\n",
        "brain_observatory_ids = list(sessions[sessions[\"session_type\"] == \"brain_observatory_1.1\"].index.unique())\n",
        "func_connectivity_ids = list(sessions[sessions[\"session_type\"] == \"functional_connectivity\"].index.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRo844emliyL"
      },
      "source": [
        "### Which sessions have all the areas in the visual cortex `VISp` `VISl` `VISpm` `VISam` `VISal` `VISrl`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUCrok4yleeG",
        "outputId": "3a5acce2-1141-417a-9daa-fd0703f10141"
      },
      "source": [
        "vis_cortex_areas = [\"VISp\", \"VISl\", \"VISpm\", \"VISam\", \"VISal\", \"VISrl\"]\n",
        "\n",
        "mask = sessions.ecephys_structure_acronyms.apply(lambda x: all(elem in x for elem in vis_cortex_areas))\n",
        "vis_cortex_ids = sessions[mask].index.values\n",
        "vis_cortex_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([719161530, 750332458, 750749662, 754312389, 755434585, 756029989,\n",
              "       778240327, 778998620, 791319847, 794812542, 797828357, 831882777,\n",
              "       847657808])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw3FNsj9ILJp"
      },
      "source": [
        "#### Make this a  `get_sessions` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MzlCpYILJp"
      },
      "source": [
        "def get_session(session_id):\n",
        "  session = cache.get_session_data(session_id)\n",
        "  return session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uv3RRCHRURd"
      },
      "source": [
        "#### Make a `get_brain_regions` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfwGgJHKRIcw"
      },
      "source": [
        "def get_brain_regions(session_id):\n",
        "  brain_regions = sessions.loc[session_id, \"ecephys_structure_acronyms\"].tolist()\n",
        "  return brain_regions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BsNC1AMSRQW"
      },
      "source": [
        "brain_regions = get_brain_regions(vis_cortex_ids[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf5Fd6hRf-Ka"
      },
      "source": [
        "## Make `get_spikes` function to load spike data into memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPUeZYzwf-Ka"
      },
      "source": [
        "The function below will load the spikes data for the specified cells and stimulus type as well as the corresponding presentations into memory.\n",
        "\n",
        "We will extract spikes using `EcephysSession.presentationwise_spike_times`, which returns spikes annotated by the `units` (`neurons`) that emitted them and the stimulus presentation during which they were emitted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OKnR5EKf-Kb"
      },
      "source": [
        "def get_spikes(session, neuron_types, stimulus_types, stimulus_table):\n",
        "\n",
        "  spikes = {}\n",
        "\n",
        "  for stimulus_type in stimulus_types:\n",
        "    scene_presentations = stimulus_table[stimulus_table[\"stimulus_name\"] == stimulus_type]\n",
        "\n",
        "    spikes[stimulus_type] = {}\n",
        "    \n",
        "    for neuron_type in neuron_types:   \n",
        "        \n",
        "      neuron_units = session.units[session.units[\"ecephys_structure_acronym\"] == neuron_type]\n",
        "\n",
        "      spikes[stimulus_type][neuron_type] = session.presentationwise_spike_times(\n",
        "                                            stimulus_presentation_ids=scene_presentations.index.values,\n",
        "                                            unit_ids=neuron_units.index.values[:])\n",
        "\n",
        "  return spikes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SZB46esNTdL"
      },
      "source": [
        "## Build Design Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nJoXOJKer7k"
      },
      "source": [
        "def build_matrix(session, stimulus_types, neuron_types, stimulus_table, features):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  design_matrix = {}\n",
        "\n",
        "  for feature in features:\n",
        "\n",
        "    if feature == \"spike count\":\n",
        "      spike_data = get_spikes(session, neuron_types, stimulus_types, stimulus_table)\n",
        "\n",
        "      for stimulus_type in tqdm_notebook(spike_data.keys()):\n",
        "        for neuron_type in spike_data[stimulus_type].keys():\n",
        "\n",
        "          spike_data[stimulus_type][neuron_type][\"count\"] = np.zeros(spike_data[stimulus_type][neuron_type] .shape[0])\n",
        "          spike_data[stimulus_type][neuron_type] = spike_data[stimulus_type][neuron_type].groupby([\"stimulus_presentation_id\", \n",
        "                                                               \"unit_id\"]).count()\n",
        "          data_matrix = pd.pivot_table(\n",
        "                                    spike_data[stimulus_type][neuron_type], \n",
        "                                    values=\"count\", \n",
        "                                    index=\"stimulus_presentation_id\", \n",
        "                                    columns=\"unit_id\", \n",
        "                                    fill_value=0.0,\n",
        "                                    aggfunc=np.sum\n",
        "                                    )\n",
        "          spike_data[stimulus_type][neuron_type] = data_matrix\n",
        "\n",
        "      design_matrix[feature] = spike_data\n",
        "      \n",
        "\n",
        "  return design_matrix\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKFnrCu3tV0q"
      },
      "source": [
        "def get_labels(design_matrix, stimulus_types):\n",
        "  \n",
        "  neuron_type = list(design_matrix[\"spike count\"][stimulus_types[0]].keys())\n",
        "  labels = {}\n",
        "  \n",
        "  for stimulus_type in tqdm_notebook(stimulus_types):\n",
        "    design_matr = design_matrix[\"spike count\"][stimulus_type][neuron_type[0]]\n",
        "    if stimulus_type == \" \":  \n",
        "      labels[stimulus_type] = stimulus_table[stimulus_table[\"stimulus_name\"] == \n",
        "                              stimulus_type].loc[design_matr.index.values, \"frame\"]\n",
        "      labels[stimulus_type] = pd.DataFrame(labels[stimulus_type])\n",
        "\n",
        "    elif stimulus_type == \"static_gratings\":\n",
        "      labels[stimulus_type] = stimulus_table[stimulus_table[\"stimulus_name\"] == \n",
        "                              stimulus_type].loc[design_matr.index.values, \"orientation\"]\n",
        "      labels[stimulus_type] = pd.DataFrame(labels[stimulus_type])\n",
        "\n",
        "  return labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPX3shPfn5tj"
      },
      "source": [
        "def get_pca(design_matrix, threshold, normalize=False):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  if normalize:\n",
        "    design_matrix = normalize(design_matrix)\n",
        "\n",
        "  \n",
        "  pca = PCA().fit(design_matrix)\n",
        "  n_components = np.sum(np.cumsum(pca.explained_variance_ratio_) <= threshold)\n",
        "  pca_model = PCA(n_components=n_components)\n",
        "  design_matrix = pca_model.fit_transform(design_matrix)\n",
        "\n",
        "  return pca_model, pd.DataFrame(design_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_R6zj6Fb_rx"
      },
      "source": [
        "def get_images(labels_df):\n",
        "\n",
        "  image_labels = list(labels_df[\"natural_scenes\"][\"frame\"].unique())\n",
        "  images = {}\n",
        "\n",
        "  if -1 in image_labels:\n",
        "    image_labels.remove(-1)\n",
        "\n",
        "  for image_index in tqdm_notebook(image_labels):\n",
        "    images[image_index] = cache.get_natural_scene_template(image_index)\n",
        "\n",
        "  return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITjXSyqqnM2F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c482c970-3529-48a1-ddfe-a5c9b090a87e"
      },
      "source": [
        "features = [\"spike count\"]\n",
        "stimulus_types = [\"natural_scenes\", \"static_gratings\"]\n",
        "neuron_types = \"VISpm\"\n",
        "design_matrix = build_matrix(session, stimulus_types, neuron_types, stimulus_table, features)\n",
        "labels_df = get_labels(design_matrix, stimulus_types)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0ad597938aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstimulus_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"natural_scenes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"static_gratings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mneuron_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"VISpm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdesign_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimulus_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimulus_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlabels_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesign_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimulus_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'session' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "VI5wloLDcCGh",
        "outputId": "baf607af-8f30-4bb8-a61b-d57ccbea3f4b"
      },
      "source": [
        "images = get_images(labels_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-00f70a703819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'labels_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEPDkblPFB_d"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISi7vcLXK94C"
      },
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import torch\n",
        "import IPython\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.models import AlexNet\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image\n",
        "from torchsummary import summary\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "resnet = torchvision.models.resnet50(pretrained=True)\n",
        "#AlexNet = torchvision.models.alexnet(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRICOgYhX-_7"
      },
      "source": [
        "## Test learned `weights` on other session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OW5mIxZlACw"
      },
      "source": [
        "### Get usual `spike_count` data. Also make `test_design_matrix` and obtain corresponding `test_labels`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En7YigVgbAsx"
      },
      "source": [
        "test_session = get_session(vis_cortex_ids[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GJBT-W4irJ"
      },
      "source": [
        "test_stimulus_table = test_session.get_stimulus_table()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3XSXpBunKoC"
      },
      "source": [
        "neuron_types = vis_cortex_area\n",
        "test_spike_data = get_spikes(test_session, neuron_types, stimulus_types, test_stimulus_table)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDBpZO5ikYcx"
      },
      "source": [
        "features = [\"spike count\"]\n",
        "stimulus_types = [\"natural_scenes\", \"static_gratings\"]\n",
        "test_design_matrix = build_matrix(test_session, stimulus_types, neuron_types, test_stimulus_table, features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtFb2oiJnPTQ"
      },
      "source": [
        "test_labels_df = get_labels(test_design_matrix, stimulus_types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7ksp_YVSpkO"
      },
      "source": [
        "test_design_matrix[\"spike count\"][\"natural_scenes\"][visual_cortex_area].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEBoOgxTvT2c"
      },
      "source": [
        "train_voxel_dims = design_array.shape[1]\n",
        "pca = PCA(n_components=train_voxel_dims)\n",
        "pca.fit(test_design_matrix[\"spike count\"][\"natural_scenes\"][visual_cortex_area])\n",
        "test_design_array = pd.DataFrame(pca.transform(test_design_matrix[\"spike count\"][\"natural_scenes\"][visual_cortex_area]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41KyI2XMEnky"
      },
      "source": [
        "test_design_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xL-w_RNHdxz"
      },
      "source": [
        "### Encoded Feature Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndzSm4bgveTh"
      },
      "source": [
        "weights = pd.read_csv(f\"weights_{vis_cortex_area}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVs0VkJ9Hh7L"
      },
      "source": [
        "encoded_feature_vectors = test_design_array.to_numpy() @ weights.T.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPK2lyfeIWfr"
      },
      "source": [
        "encoded_feature_vectors = pd.DataFrame(encoded_feature_vectors, index=stim_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RQzW0wVI3Zx"
      },
      "source": [
        "encoded_feature_vectors = encoded_feature_vectors.drop(nan_obsIDs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH8JjE0qtXuZ"
      },
      "source": [
        "encoded_feature_vectors.to_csv(f\"encoded_feature_vectors_{vis_cortex_area}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwYRmK8P72eD"
      },
      "source": [
        "encoded_feature_vectors.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zemP05z6N55j"
      },
      "source": [
        "encoded_feature_vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QX84-MEEn3P"
      },
      "source": [
        "# Reconstruct Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZB5pbw_J75J"
      },
      "source": [
        "### Create `VAE_decoder` class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3UXWX0sEqEN"
      },
      "source": [
        "class VAE_decoder(nn.Module):\n",
        "    def __init__(self, feat_size, output_dim=(1, 256, 256)):\n",
        "        \"\"\"\n",
        "        Initializes the VAE decoder network.\n",
        "        Optional args:\n",
        "        - feat_size (int): size of the final features layer (default: 256)\n",
        "        - output_dim (tuple): output image dimensions (channels, width, height) \n",
        "            (default: (1, 256, 256))\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.feat_size = feat_size\n",
        "        self._vae = True\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.decoder_linear = nn.Sequential(\n",
        "              nn.Linear(self.feat_size, 512),\n",
        "              nn.ReLU(),\n",
        "              nn.BatchNorm1d(512, affine=False),\n",
        "              nn.Linear(512, 1024),\n",
        "              nn.ReLU(),\n",
        "              nn.BatchNorm1d(1024, affine=False),\n",
        "              nn.Linear(1024, 59536),\n",
        "              nn.ReLU()\n",
        "        )\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "              nn.UpsamplingNearest2d(scale_factor=2),\n",
        "              nn.BatchNorm2d(16, affine=False),\n",
        "              nn.ConvTranspose2d(\n",
        "                  in_channels=16, out_channels=6, kernel_size=5, stride=1\n",
        "                  ),\n",
        "              nn.ReLU(),\n",
        "              nn.UpsamplingNearest2d(scale_factor=2),\n",
        "              nn.BatchNorm2d(6, affine=False),\n",
        "              nn.ConvTranspose2d(\n",
        "                  in_channels=6, out_channels=1, kernel_size=5, stride=1\n",
        "                  )\n",
        "        )\n",
        "\n",
        "        self._test_output_dim()\n",
        "\n",
        "    @property\n",
        "    def vae(self):\n",
        "        return self._vae\n",
        "\n",
        "    def _test_output_dim(self):\n",
        "        dummy_tensor = torch.ones(1, self.feat_size)\n",
        "        reset_training = self.training\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            decoder_output_shape = self.reconstruct(dummy_tensor).shape[1:]\n",
        "        if decoder_output_shape != self.output_dim:\n",
        "            raise ValueError(f\"Decoder produces output of shape \"\n",
        "                f\"{decoder_output_shape} instead of expected \"\n",
        "                f\"{self.output_dim}.\")\n",
        "        if reset_training:\n",
        "            self.train()\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = self.decoder_linear(z.float())\n",
        "        h3 = h3.view(-1, 16, 61, 61)\n",
        "        recon_x_logits = self.decoder_conv(h3)\n",
        "        return recon_x_logits\n",
        "\n",
        "    def forward(self, X):\n",
        "        z = X\n",
        "        recon_x_logits = self.decode(z)\n",
        "        return recon_x_logits\n",
        "\n",
        "    def reconstruct(self, X):\n",
        "        with torch.no_grad():\n",
        "            recon_x = torch.sigmoid(self.decode(X))\n",
        "        return recon_x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLs94KNTKFFZ"
      },
      "source": [
        "#### Specify VAE `loss_function`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9s3vR14JtYD"
      },
      "source": [
        "def vae_loss_function(recon_X_logits, X, beta=1.0):\n",
        "    \"\"\"\n",
        "    vae_loss_function(recon_X_logits, X, mu, logvar)\n",
        "    Returns the weighted VAE loss for the batch.\n",
        "    Required args:\n",
        "    - recon_X_logits (4D tensor): logits of the X reconstruction \n",
        "        (batch_size x shape of x)\n",
        "    - X (4D tensor): X (batch_size x shape of x)\n",
        "    - mu (2D tensor): mu values (batch_size x number of features)\n",
        "    - logvar (2D tensor): logvar values (batch_size x number of features)\n",
        "    Optional args:\n",
        "    - beta (float): parameter controlling weighting of KLD loss relative to \n",
        "        reconstruction loss. (default: 1.0)\n",
        "    \n",
        "    Returns:\n",
        "    - (float): weighted VAE loss\n",
        "    \"\"\"\n",
        "\n",
        "    BCE = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "        recon_X_logits, X, reduction=\"sum\"\n",
        "        )\n",
        "    #KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    return BCE\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FqRxID2KLej"
      },
      "source": [
        "#### Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PilStV7RJywd"
      },
      "source": [
        "def train_vae(encoder, labels_df, image_dataset, dataset, train_sampler, num_epochs=10, batch_size=500, \n",
        "              beta=1.0,  use_cuda=True, verbose=False):\n",
        "    \"\"\"\n",
        "    train_vae(encoder, dataset, train_sampler)\n",
        "    Function to train an encoder using the SimCLR loss.\n",
        "    \n",
        "    Required args:\n",
        "    - encoder (nn.Module): Encoder network instance for extracting features. \n",
        "        Should have method get_features().\n",
        "    - dataset (dSpritesTorchDataset): dSprites torch dataset\n",
        "    - train_sampler (SubsetRandomSampler): Training dataset sampler.\n",
        "    \n",
        "    Optional args:\n",
        "    - num_epochs (int): Number of epochs over which to train the classifier. \n",
        "        (default: 10)\n",
        "    - batch_size (int): Batch size. (default: 100)\n",
        "    - beta (float): parameter controlling weighting of KLD loss relative to \n",
        "        reconstruction loss. (default: 1.0)\n",
        "    - use_cuda (bool): If True, cuda is used, if available. (default: True)\n",
        "    - verbose (bool): If True, 5 first batch reconstructions are plotted at \n",
        "        each epoch. (default: False)\n",
        "    Returns: \n",
        "    - encoder (nn.Module): trained encoder\n",
        "    - decoder (nn.Module): trained decoder\n",
        "    - loss_arr (list): training loss at each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    decoder = VAE_decoder(encoded_feature_vectors.shape[1] , (1, 256, 256)).to(device)\n",
        "\n",
        "    # if not encoder.vae:\n",
        "    #     raise ValueError(\"Must pass encoder for which self.vae is True.\")\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        torch.tensor(encoded_feature_vectors.to_numpy()), batch_size=batch_size,\n",
        "        )\n",
        "\n",
        "    \n",
        "    image_dataloader = torch.utils.data.DataLoader(\n",
        "                        image_dataset, batch_size=batch_size)\n",
        "\n",
        "    dataiter = iter(image_dataloader)\n",
        "    images_loader, labels = dataiter.next()\n",
        "\n",
        "\n",
        "    # Define loss and optimizers\n",
        "    # train_params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "    optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=500\n",
        "        )\n",
        "\n",
        "    # Train model on training set\n",
        "    # reset_encoder_training = encoder.training\n",
        "    # encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    loss_arr = []\n",
        "    for epoch in tqdm_notebook(range(num_epochs)):\n",
        "        total_loss = 0\n",
        "        num_total = 0\n",
        "        for batch_idx, X in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            recon_X_logits = decoder(X.to(device))\n",
        "\n",
        "\n",
        "            start_idx = int(batch_idx*batch_size)\n",
        "            end_idx = int(batch_size*(batch_idx + 1) - 1)\n",
        "\n",
        "            if batch_idx == len(train_dataloader) - 1:\n",
        "              image_labels = labels_df.loc[start_idx:, \"frame\"].tolist()\n",
        "            else: \n",
        "              image_labels = labels_df.loc[start_idx: end_idx, \"frame\"].tolist()\n",
        "            \n",
        "            ims = np.zeros((X.shape[0], 1, 256, 256))\n",
        "\n",
        "            for indx, image_label in enumerate(image_labels):\n",
        "              ims[indx, :, :, :] = images_loader[int(image_label)].sum(0).unsqueeze(0)\n",
        "            ims = torch.Tensor(ims)\n",
        "\n",
        "\n",
        "            loss = vae_loss_function(\n",
        "                recon_X_logits=recon_X_logits, X=ims.to(device), beta=beta\n",
        "                )\n",
        "            total_loss += loss.item()\n",
        "            num_total += len(recon_X_logits)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if verbose and epoch % 10 == 9 and batch_idx == 0:\n",
        "                num_images = 5\n",
        "                # encoder.eval()\n",
        "                decoder.eval()\n",
        "                with torch.no_grad():\n",
        "                    input_imgs = X[:num_images].detach().cpu().numpy()\n",
        "                    output_imgs = decoder.reconstruct(\n",
        "                                  input_imgs.to(device))\\\n",
        "                                  .detach().cpu().numpy()\n",
        "                # encoder.train()\n",
        "                decoder.train()\n",
        "\n",
        "                title = (f\"Epoch {epoch}, batch {batch_idx}, \"\n",
        "                    f\"loss {loss.item():.2f}\")\n",
        "                plot_util.plot_dsprite_image_doubles(\n",
        "                    list(input_imgs), list(output_imgs), \"Reconstr.\",\n",
        "                    title=title)\n",
        "\n",
        "        loss_arr.append(total_loss / num_total)\n",
        "        scheduler.step()\n",
        "\n",
        "    # set final decoder state and reset original encoder state\n",
        "    decoder.train()\n",
        "    decoder.cpu()\n",
        "    # if reset_encoder_training:\n",
        "    #     encoder.train()\n",
        "    # else:\n",
        "    #     encoder.eval()\n",
        "    # encoder.to(reset_encoder_device)\n",
        "\n",
        "    return decoder, loss_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S9EMIzwk8_d"
      },
      "source": [
        "images = get_images(labels_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk5_8FTeCfeC"
      },
      "source": [
        "parent_dir = 'decoding_datasets'\n",
        "\n",
        "transformations = [transforms.ToTensor(),                 \n",
        "                  transforms.Resize((256, 256)),\n",
        "                  transforms.ToPILImage()\n",
        "                  ]\n",
        "\n",
        "image_labels = list(labels_df[\"natural_scenes\"][\"frame\"].unique())\n",
        "\n",
        "if -1 in image_labels:\n",
        "  image_labels.remove(-1)\n",
        "\n",
        "for image_index in tqdm_notebook(image_labels):\n",
        "\n",
        "  directory = f\"image{image_index}\"\n",
        "  path = os.path.join(parent_dir, directory)\n",
        "\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path) \n",
        "  \n",
        "  im = Image.fromarray(images[image_index])\n",
        "  transform = transforms.Compose(transformations)\n",
        "  transformed_image = transform(im)\n",
        "  im = transform(im)\n",
        "  im.save(path+f'/image_{image_index}.jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVM16YldCts9"
      },
      "source": [
        "image_dataset = ImageFolder('decoding_datasets',\n",
        "                              transform=train_transform)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTs0PtJ9S1jM"
      },
      "source": [
        "temp_labels_df = labels_df[\"natural_scenes\"][labels_df[\"natural_scenes\"][\"frame\"] != -1].reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8UV_AMsSEH8"
      },
      "source": [
        "decoder, loss = train_vae(encoded_feature_vectors, temp_labels_df, image_dataset, dataset=None, train_sampler=None, num_epochs=10, \n",
        "                          batch_size=500, beta=1.0,  use_cuda=True, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}